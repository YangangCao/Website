\documentclass[10pt,a4paper,oneside]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{enumerate}
\usepackage{graphicx}

\author{Baboo J. Cui, Yangang Cao}
\title{Linear Quadratic Regulation for Continuous-Time System}
\begin{document}
\maketitle
\tableofcontents

\newpage
\section{LQR for CT Case Summary}

\subsection{Continuous-Time LQR Problem Formulation}
Given a continuous-time LTI system
\[
\dot{x} = Ax+Bu,\ \ x(0)=x_0
\]
Given a time horizon $t \in [0, t_f]$, find the optimal input $u(t)$, $t \in [0, t_f]$, that minimizes the cost function
\[
J(u)=\underbrace{\int_{0}^{t_{f}} \left(x^{T}(t) Q x(t)+u^{T}(t) R u(t)\right) dt}_{\text { running cost }} +\underbrace{x^{T}\left(t_{f}\right) Q_{f} x\left(t_{f}\right)}_{\text { terminal cost }}
\]
This is similar to DT case, the sum becomes integration, again:
\begin{itemize}
\item \textbf{state weight matrix}: $Q=Q^T \succeq 0$
\item \textbf{control weight matrix}: $R=R^T\succ 0$
\item \textbf{final state weight matrix}: $Q_f=Q^T_f \succeq 0$ 
\item \textbf{time horizon}: $t_f$(could be infinity)
\end{itemize}

\subsection{Value Function of CT LQR Problem}
The value function at time $t\in[0,t_f]$ is:
\[
V_{t}(x)=\min _{u(\tau), \tau \in \left[t, t_{f}\right]} \int_{t}^{t_{f}} \left(x^{T} (\tau) Q x(\tau)+u^{T}(\tau) R u(\tau)\right) d \tau + x^{T}\left(t_{f}\right) Q_{f} x\left(t_{f}\right)
\]
where $x \in \mathbb{R}^n$ is the state vector with the initial condition $x(t)=x$(here $x$ can be any feasible vector). Note that:
\begin{itemize}
\item optimal cost of LQR solve for a shorter time horizon $[t,t_f]$
\item optimal cost-to-go assumes the state starts from $x$ at time $t$
\item $V_0(x_0)$ is the optimal cost of the original LQR problem
\end{itemize}

\subsection{Solution Overview(VIP)}
\begin{itemize}
\item value function at terminal time obviously is quadratic: $V_{t_f}(x)=x^T Q_f x$
\item value function at any time $t \in [0,t_f]$ is also \textbf{quadratic}:
\[
V_t(x)=x^T P(t) x
\]
can be proved by the way for DT case
\item value functions at different times are related by the (\textbf{CT Riccati}) matrix differential equation(on each term in the matrix)
\[
\dot{-P(t)}=Q + A^T P(t) + P(t)A - P(t)BR^{-1}B^T P(t)
\]
\item integrating the differential equation backward in time yields $P(0)$
\item minimum cost to the original problem is given by $V_0(x_0)=x^T_0 P(0) x_0$
\item optimal control is a linear state feedback controller:
\[
u^{*}(t)=-R^{-1} B^{T} P(t) x^{*}(t)
\]
tip: in DT case, gain depends on $P$ of next time, here the gain depend on current time $P_t$
\end{itemize}

\section{Derivation of Value Functions}
\subsection{Linear Approximation}
Assume the system state starts from $x$ at time $t$, namely $x(t) = x$ and assume the control input is kept \textbf{constant} briefly in a very short period time $\delta$:
\[
u(s) \equiv w, \quad \forall s \in[t, t+\delta]
\]
Attention: here $\delta \in \mathbb{R}$ is a small \textbf{scalar} instead of an operator. Then:
\begin{align*}
x(t+\delta) &= e^{A \delta} x(t)+\int_{t}^{t+\delta} e^{A(t+\delta-\tau)} B u(\tau) d \tau \quad \text{analytic solution}\\ 
&\simeq x+\delta(A x+B w) \quad \text{\textbf{linear approximation}}
\end{align*}

\subsection{Dynamic Programming Principle}
\textbf{Bellman equation}: the (optimal) cost-to-go at time $t$ from $x$ is
\[
V_{t}(x) \simeq \min _{w}\left[\underbrace{\delta\left(x^{T} Q x+w^{T} R w\right)}_{\text { running cost during }[t, t+\delta)}+\underbrace{V_{t+\delta}(x+\delta(A x+B w))}_{\text { cost-to-go from time } t+\delta}\right]
\]
For the first term(linear approximation):
\[
\delta\left(x^{T} Q x+w^{T} R w\right) \approx \int_{t}^{t+ \delta} \left(x^{T} (\tau) Q x(\tau)+u^{T}(\tau) R u(\tau)\right) d \tau
\]
For the second term:
\[
V_{t+\delta}(x+\delta(A x+B w)) = V_{t+\delta}(x+\delta \dot{x}_t) \approx V_{t+\delta}(x_{t+\delta})
\]
expand and let $\delta \rightarrow 0$, we have
\[
-x^{T} \dot{P}(t) x=\min _{w}\left\{x^{T} Q x+w^{T} R w+x^{T} P(t)(A x+B w)+(A x+B w)^{T} P(t) x\right\}
\]

\section{Conclusions}
Here is the list of conclusions.

\subsection{Continuous-Time Riccati Equation}
The optimal control for state $x$ at time $t$ is
\[
u^{*}(t)=w^{*}=-K(t) x=
-\underbrace{R^{-1} B^{T} P(t)}_{\text { Kalman gain }} x
\]
and $P(t)$ satisfies the {\bfseries continuous-time Riccati differential equation}
$$
-\dot{P}(t)=Q+P(t) A+A^{T} P(t)-P(t) B R^{-1} B^{T} P(t), 0 \leq t \leq t_{f}
$$
with (terminal) condition $P(t_f)=Q_f$. This is not easy to solve because nonlinear term $P(t) B R^{-1} B^{T} P(t)$. A CT system can be converted to DT to solve for simplicity.

\subsection{CT LQR Solution Algorithm Summary}

\begin{enumerate}
\item set $P(t_f)=Q_f$
\item solve the Riccati differential equation backward in time:
\[
-\dot{P}(t)=Q + A^{T} P(t) + P(t) A - P(t) B R^{-1} B^{T} P(t)
\]
\item return $V_0(x_0)=x_0^TP(0)x_0$ as the optimal cost
\item solve forward in time the closed-loop system dynamics under the linear state feedback control:
\[
u(t)=-K(t)x(t)
\]
with optimal system dynamic:
\[
\dot{x}^{*}(t)=(A-B K(t)) x^{*}(t), \quad x^{*}(0)=x_{0}
\]
where $K(t)$ is the Kalman gain $K(t)=R^{-1}B^T P(t)$
\item return $x^*(t)$ as the optimal state trajectory and return $u^*(t)=-K(t)x^*(t)$ as the optimal control input
\end{enumerate}

\section{Infinite Horizon Problem}
Find the optimal control $u(t),t \geq 0$, to minimize
\[
\int_{0}^{\infty}\left(x^{T}(t) Q x(t)+u^{T}(t) R u(t)\right) dt
\]
subject to system dynamic constraints:
\[
\dot{x} = Ax + Bu, x(0)=x_0
\]
Again state weight $Q \succeq 0$ and control weight $R \succ 0$, and there is no terminal cost. And value function is:
\[
V(x) = \min _{u} \int_{0}^{\infty} \left(x^{T}(t) Q x(t)+u^{T}(t) R u(t)\right) dt
\]
\begin{itemize}
\item value function is independent of the starting time(steady state)
\item optimal cost of the original problem: $V(x_0)$
\end{itemize}
If $(A, B)$  is stabilizable, then $V(x)=x^{T} P x$ for some $P=P^{T} \succ 0$ is a \textbf{finite} quadratic function, and the optimal control is a \textbf{static} state feedback control:
\[
u(t)=-Kx(t)
\]
where 
\[
K=R^{-1}B^T P
\]
\begin{itemize}
\item $P$ solves the \textbf{continuous-time Algebraic Riccati Equation(CARE)}
\[
Q+P A+A^{T} P -P B R^{-1} B^{T} P = 0
\]
\item $P$ can be approximated by solving the LQR problem over sufficiently large time horizon (with $Q_f=0$), or by Matlab command \textit{care()}
\end{itemize}
If $(A, B)$  is stabilizable and $Q =C^T C$ with $(C,A)$ detectable, then closed-loop system $A-BK$ under the optimal control $u=-Kx$ is stable.

\section{Alternative Solution by Lagrange Multiplier}
Finite horizon LQR problem can be considered as \textbf{constrained optimization problem}:
\begin{align*}
\min _{u} J(u) &=\frac{1}{2} \int_{0}^{t_{f}}\left(x^{T}(t) Q x(t)+u^{T}(t) R u(t)\right) dt + \frac{1}{2} x^{T}\left(t_{f}\right) Q_{f} x\left(t_{f}\right)\\
\text{s.t.} \quad \dot{x}(t) &= Ax(t)+Bu(t),t\in[0,t_f]\\
\end{align*}
\begin{itemize}
\item optimization variables are $u(t),t \in [0, t_f]$
\item there are \textbf{infinite} number of equality constraints, one for each $t\in[0,t_f]$
\end{itemize}
Convert the above problem to \textbf{unconstrained optimization problem} by \textbf{Lagrange} method:
\[
L(u, x, \lambda)= J(u) + \int_{0}^{t_{f}} \lambda^{T}(t)(A x(t)+B u(t)-\dot{x}(t)) dt
\]

\begin{itemize}
\item \textbf{Lagrange multiplier function}:$\lambda:[0,t_f]\rightarrow\mathbb{R}^n$, same dimension as state variable, infinite number of $\lambda$ for each $t$
\item original problem solution:
\[
\min _{u} J(u)=\min _{u, x} \max _{\lambda} L(u, x, \lambda) \geq \max _{\lambda} \min _{u, x} L(u, x, \lambda)
\]
this is \textbf{strong duality} and detailed info are covered in \textbf{game theory}. A quick intuitive way to see this is big $\lambda$ force the state trajectory in track while at the same time treat $x$ and $u$ independently
\end{itemize}
Optimal solution $(u^*,x^*,\lambda^*)$ must satisfy:
\[
\frac{\partial L}{\partial u}=0 \quad \text{and} \quad \frac{\partial L}{\partial x}=0
\]
Use integration by part to rewrite $L$ as:
$$
L=J(u)+\int_{0}^{t_{f}}\left[\lambda(t)^{T}(A x(t)+B u(t))+\dot{\lambda}(t)^{T} x(t)\right] d t-\lambda(t)^{T} x\left.(t)\right|_{0} ^{t_{f}}
$$
\textbf{Needle-like variations}: for each $t\in[0,t_f]$, perturb $u(t)$ and $x(t)$ locally:
\begin{align*}
\nabla_{u(t)} L=R u(t)+B^{T} \lambda(t)=0 &  \quad\Rightarrow \quad u(t)=-R^{-1} B^{T} \lambda(t) \\
\nabla_{x(t)} L=Q x(t)+A^{T} \lambda(t)+\dot{\lambda}(t)=0 &  \quad\Rightarrow \quad \dot{\lambda}(t)=-A^{T} \lambda(t)-Q x(t) \\ 
\nabla_{x\left(t_{f}\right)} L=Q_{f} x\left(t_{f}\right)-\lambda\left(t_{f}\right)=0 &  \quad\Rightarrow \quad \lambda\left(t_{f}\right)=Q_{f} x\left(t_{f}\right) 
\end{align*}
$\lambda$ is called the \textbf{co-state}, and satisfies the \textbf{co-state equation}:
\[
\dot{\lambda}(t)=-A^{T} \lambda(t)-Q x(t), \quad t \in\left[0, t_{f}\right]
\]
with terminal boundary condition $\lambda(t_f)=Q_fx(t_f)$.
\begin{itemize}
	\item can be adopted for state or input constraints
	\item known as Pontryagin's maximum principle
\end{itemize}

\section{Hamiltonian Equation}
The optimal control $u^*(t)$ is given by
\[
u^{*}(t)=-R^{-1} B^{T} \lambda^{*}(t), \quad t \in\left[0, t_{f}\right]
\]
while the optimal state $x^*$ and co-state $\lambda^*$ satisfies
\[
\frac{d}{d t}\left[\begin{array}{l}{x^{*}(t)} \\ {\lambda^{*}(t)}\end{array}\right]=\underbrace{\left[\begin{array}{cc}{A} & {-B R^{-1} B^{T}} \\ {-Q} & {-A^{T}}\end{array}\right]}_{\text { Hamiltonian }}\left[\begin{array}{c}{x^{*}(t)} \\ {\lambda^{*}(t)}\end{array}\right], \quad t \in\left[0, t_{f}\right]
\]
with two-point boundary condition: $x^{*}(0)=x_{0}, \lambda^{*}\left(t_{f}\right)=Q_{f} x^{*}\left(t_{f}\right)$
\begin{itemize}
\item two-point boundary value problem
\item solved numerically using the shooting method
\item nice form that a linear system govern both $x$ and $\lambda$
\item use shooting method to solve it(adaptive guess)
\end{itemize}

\subsection{Connecting Riccati and Hamiltonian Solutions}
\begin{itemize}
\item dynamic programming method says $u^{*}(t)=-R^{-1} B^{T} P(t) x^{*}(t)$ where $P(t)$ solves the Riccati differential equation
$$
-\dot{P}(t)=Q+P(t) A+A^{T} P(t)-P(t) B R^{-1} B^{T} P(t), P\left(t_{f}\right)=Q_{f}
$$
\item variational method says that $u^{*}(t)=-R^{-1} B^{T} \lambda^{*}(t)$ where $\lambda^*(t)$ solves the co-state equation
$$
\dot{\lambda}^{*}(t)=-A^{T} \lambda^{*}(t)-Q x^{*}(t), \quad \lambda^{*}\left(t_{f}\right)=Q_{f} x^{*}\left(t_{f}\right)
$$
\item a natural guess is 
$$
\lambda^{*}(t)=P(t) x^{*}(t), \quad t \in\left[0, t_{f}\right]
$$ 
\end{itemize}
Indeed, this is the case: if $P(t)$ solves the Riccati equation, then
\[
\lambda^{*}(t) :=P(t) x^{*}(t)
\]
must solve co-state equation.

\subsection{Matrix Hamiltonian Equations}
Consider the matrix Hamiltonian differential equation
$$
\frac{d}{d t}\left[\begin{array}{l}{X(t)} \\ {Y(t)}\end{array}\right]=\left[\begin{array}{cc}{A} & {-B R^{-1} B^{T}} \\ {-Q} & {-A^{T}}\end{array}\right]\left[\begin{array}{l}{X(t)} \\ {Y(t)}\end{array}\right]
$$
where $X(t),Y(t)\in\mathbb{R}^{n\times n}$, Suppose $X(t),Y(t)\in\mathbb{R}^{n\times n}$ solve the matrix Hamiltonian differential equation with boundary condition 
\[
X(t_f)=I \quad \text{and} \quad Y(t_f)=Q_f
\] 
Then $P(t) :=Y(t) X(t)^{-1}$ is the solution to Riccati differential equation. Hence the (nonlinear) Riccati differential equation can be solved via solving the (linear) matrix Hamiltonian differential equation(good for numerical calculation).

\end{document}