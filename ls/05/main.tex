\documentclass[10pt,a4paper,oneside]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{enumerate}
\usepackage{multirow}
% \usepackage[framed,numbered,autolinebreaks,useliterate]{mcode} 
\date{July 9, 2019}
\author{Baboo J. Cui, Yangang Cao}
\title{Matrix Exponential}
\newcommand{\tabincell}[2]{\begin{tabular}{@{}#1@{}}#2\end{tabular}}
\begin{document}
\maketitle
\tableofcontents
\newpage
\section{Matrix Exponential}
Power series converges for all $\lambda\in\mathbb{R}$ in scalar exponential function:
\[
e^{\lambda}=1+\lambda+\frac{1}{2 !} \lambda^{2}+\frac{1}{3 !} \lambda^{3}+\cdots
\]
For any matrix $A \in \mathbb{R}^{n \times n}$, \textbf{define} its matrix exponential:
\[
e^{A} :=I_{n}+A+\frac{1}{2 !} A^{2}+\frac{1}{3 !} A^{3}+\cdots \in \mathbb{R}^{n \times n}
\]
and matrix power series \textbf{always converges}.

\subsection{Computing Matrix Exponential Directly}
\begin{itemize}
\item When $A$ is nilpotent:
\[
A^n = \mathbf{0}, n>N \text{ where $N$ is certain number}
\]
therefore $e^A$ only depends on the first $N$ terms
\item When $A$ is idempotent: $A^{2}=A$, there is an analytic solution:
\[
e^A = I + (e-1)A
\]
\item When $A$ is of rank one, simply use Dyadic expansion:
\[
A = uv^T
\]
\end{itemize}

\subsection{Computing Matrix Exponential by Jordan Form}
Using the Jordan Canonical Form:
\[
A=T\left[\begin{array}{lll}{J_{1}} & {} \\ {} & {\ddots} & {} \\ {} & {} & {J_{q}}\end{array}\right] T^{-1} \Rightarrow e^{A}=T\left[\begin{array}{ccc}{e^{J_{1}}} & {} & {} \\ {} & {\ddots} & {} \\ {} & {} & {e^{J_{q}}}\end{array}\right] T^{-1}
\]
\subsection{Computing Matrix Exponential by Matlab}
Use Matlab command:
\[
\mathbf{expm}()
\]
note there is an "m".

\section{Properties of Matrix Exponential}
For any $A\in\mathbb{R}^{n\times n}$
\begin{itemize}
\item $e^{\mathbf{0}}=I$
\item invariant of eigenvalue: $A v=\lambda v \quad \Rightarrow \quad e^{A} v=e^{\lambda} v$
\item $e^{A^{T}}=\left(e^{A}\right)^{T}$
\item $e^{T A T^{-1}}=T e^{A} T^{-1}$ for nonsingular $T \in \mathbb{R}^{n \times n}$
\item $\operatorname{det}\left(e^{A}\right)=e^{\operatorname{tr} A} = e^{\lambda_1 + \lambda_2 + \cdots}$
\item If $A, B \in \mathbb{R}^{n \times n}$ commute, i.e., $A B=B A,$ then
\[
e^{A+B}=e^{A} e^{B}=e^{B} e^{A}
\]
Tip: commute indicate that eigenvectors of both $A$ and $B$ are in the same direction
\item $\left(e^{A}\right)^{-1}=e^{-A}$
\item If $A$ is skew symmetric $\left(A^{T}=-A\right), e^{A}$ is orthogonal: $\left(e^{A}\right)\left(e^{A}\right)^{T}=I$
\end{itemize}

\section{Baker-Campbell-Hausdorff Formula}
For $X, Y \in \mathbb{R}^{n \times n},$ we have 
\[
e^{X+Y} \neq e^{X} \cdot e^{Y}
\] 
unless $X$ and $Y$ commute. For any $X, Y \in \mathbb{R}^{n \times n},$ we can write
\[
e^{X} e^{Y}=e^{Z}
\]
for some $Z=\log \left(e^{X} e^{Y}\right) \in \mathbb{R}^{3 \times 3}$ given by
\[
Z=X+Y+\frac{1}{2}[X, Y]+\frac{1}{12}[X,[X, Y]]-\frac{1}{12}[Y,[X, Y]]-\frac{1}{24}[Y,[X,[X, Y]]]-\cdots
\]
where $[X, Y] :=X Y-Y X$ is the \textbf{Lie bracket} of $X$ and $Y$.
\section{Matrix Exponential Representation of 3D Rotations}
For $\omega=\left[\omega_{1} \quad \omega_{2} \quad \omega_{3}\right]^{T} \in \mathbb{R}^{3},$ define a skew-symmetric matrix $\Omega$:
\[
\Omega=\left[\begin{array}{ccc}{0} & {-\omega_{3}} & {\omega_{2}} \\ {\omega_{3}} & {0} & {-\omega_{1}} \\ {-\omega_{2}} & {\omega_{1}} & {0}\end{array}\right] \in \mathbb{R}^{3 \times 3}
\]
then $\Omega_V=\omega \times v$ for $v \in \mathbb{R}^{3},$ where $\times$ denotes cross product of vectors. For any nonzero vector $\omega \in \mathbb{R}^{3}, e^{\Omega} \in \mathbb{R}^{3 \times 3}$ is an orthogonal matrix that represents the rotation around the axis $\omega$ by the angle $||\omega||$.  More precisely,
\[
e^{\Omega}=I_{3}+\frac{\sin (\|\omega\|)}{\|\omega\|} \Omega+\frac{1-\cos (\|\omega\|)}{\|\omega\|^{2}}\left(\omega \omega^{T}-\|\omega\|^{2} I_{3}\right)
\]
Here is an example: 
\[
A= \left[\begin{array}{cc}{\sigma} & {-\omega} \\ {\omega} & {\sigma}\end{array}\right] = \left[\begin{array}{cc}{\sigma} & {0} \\ {0} & {\sigma}\end{array}\right] + \left[\begin{array}{cc}{0} & {-\omega} \\ {\omega} & {0}\end{array}\right]
\]
therefore:
\[
e^A = \underbrace{e^\sigma}_{\text{magnitude change}} \underbrace{ \left[\begin{array}{cc}{\cos \omega} & {-\sin \omega} \\ {\sin \omega} & {\cos \omega}\end{array}\right]}_{\text{pure rotation}}
\]

\section{Time Indexed Case}

\subsection{Time Indexed Matrix Exponential}
The following power series converges for all $\lambda \in \mathbb{R}$ and all $t \in \mathbb{R}$:
\[
f(\lambda) :=e^{\lambda t}=1+t \lambda+\frac{1}{2 !} t^{2} \lambda^{2}+\frac{1}{3 !} t^{3} \lambda^{3}+\cdots
\]
For any square matrix $A\in\mathbb{R}^{n\times n}$, define
\[
e^{A t} :=I_{n}+t A+\frac{1}{2 !} t^{2} A^{2}+\frac{1}{3 !} t^{3} A^{3}+\cdots
\]
\begin{itemize}
\item The matrix power series converges for all $A \in \mathbb{R}^{n \times n}$ and all $t \in \mathbb{R}$
\item For fixed $A, e^{A t}$ can be viewed as a matrix-valued function of time $t$
\end{itemize}

\subsection{Time Derivative of Matrix Exponential}
The scalar function $e^{\lambda t}$ as \textbf{a function of} $t \in \mathbb{R}$ has the derivative:
\[
\frac{d}{d t} e^{\lambda t}=\lambda e^{\lambda t}
\]
For fixed $A \in \mathbb{R}^{n \times n}, e^{A t}$ as a matrix-valued function of $t \in \mathbb{R}$ satisfies
\[
\frac{d}{d t} e^{A t}=A \cdot e^{A t}=e^{A t} \cdot A
\]
Tip: why this is commute?

\subsection{Properties of Matrix Exponential with Time Index}
For any $A \in \mathbb{R}^{n \times n}$ and any $t \in \mathbb{R}$:
\begin{itemize}
\item $A v=\lambda v \quad \Rightarrow \quad e^{A t} v=e^{\lambda t} v$
\item $e^{A^{T} t}=\left(e^{A t}\right)^{T}$
\item $\operatorname{det}\left(e^{A t}\right)=e^{(\operatorname{tr} A) t}$
\item If $A, B \in \mathbb{R}^{n \times n}$ commute, i.e., $A B=B A,$ then
\[
e^{(A+B) t}=e^{A t} e^{B t}=e^{B t} e^{A t}
\]
\item $e^{A\left(t_{1}+t_{2}\right)}=e^{A t_{1}} e^{A t_{2}}=e^{A t_{2}} e^{A t_{1}}, \forall t_{1}, t_{2} \in \mathbb{R}$
\item $\left(e^{A t}\right)^{-1}=e^{-A t}$
\item If $A$ is skew symmetric, then $e^{A t}$ is orthogonal for all $t$
\end{itemize}

\subsection{Computing Time-Indexed Matrix Exponential}
Three methods can be taken:
\begin{itemize}
	\item use the \textbf{definition}:
	\[
	e^{A t} :=I_{n}+t A+\frac{1}{2 !} t^{2} A^{2}+\frac{1}{3 !} t^{3} A^{3}+\cdots
	\]
	\item  use the \textbf{Jordan canonical form}:
	\[
	A=T\left[\begin{array}{lll}{J_{1}} & {} & {} \\ {} & {\ddots} & {} \\ {} & {} & {J_{q}}\end{array}\right] T^{-1} \quad \Rightarrow \quad e^{A t}=T\left[\begin{array}{ccc}{e^{J_{1} t}} & {} & {} \\ {} & {\ddots} & {} \\ {} & {} & {e^{J_{q} t}}\end{array}\right] T^{-1}
	\]
	\item the Laplace transform of $e^{At}$ as a function of time $t$ is
	\[
	\mathcal{L}\left[e^{A t}\right]=(sI-A)^{-1} \quad \Rightarrow \quad e^{A t} = \mathcal{L}^{-1}\left[(sI-A)^{-1}\right]
	\]
\end{itemize}
Here are two examples:
\[
A_{1}=\left[\begin{array}{cc}{0} & {-\omega} \\ {\omega} & {0}\end{array}\right], \quad e^{A_{1} t}=\left[\begin{array}{cc}{\cos(\omega t)} & {-\sin(\omega t)} \\ {\sin(\omega t)} & {\cos (\omega t)}\end{array}\right]
\]
\[
A_{2}=\left[\begin{array}{ccc}{1} & {1} & {0} \\ {0} & {-2} & {1} \\ {0} & {0} & {-2}\end{array}\right],\left(sI-A_{2}\right)^{-1}=\left[\begin{array}{ccc}{\frac{1}{s-1}} & {\frac{1}{(s-1)(s+2)}}&{\frac{1}{(s-1)(s+2)^{2}}} \\ {0} & {\frac{1}{s+2}}&  {\frac{1}{(s+2)^2}} \\ {0} & {0}& {\frac{1}{s+2}} \end{array}\right]
\]
\end{document}