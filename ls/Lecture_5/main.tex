\documentclass[10pt,a4paper,oneside]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{enumerate}
\usepackage{multirow}
\usepackage[framed,numbered,autolinebreaks,useliterate]{mcode} 
\date{July 9, 2019}
\author{Baboo J. Cui, Yangang Cao}
\title{Lecture 4: Matrix Exponential}
\newcommand{\tabincell}[2]{\begin{tabular}{@{}#1@{}}#2\end{tabular}}
\begin{document}
\maketitle
\tableofcontents
\newpage
\section{Matrix Exponential}
Power series converges for all $\lambda\in\mathbb{R}$:
\[
e^{\lambda}=1+\lambda+\frac{1}{2 !} \lambda^{2}+\frac{1}{3 !} \lambda^{3}+\cdots
\]
For any matrix $A \in \mathbb{R}^{n \times n},$ define its matrix exponential:
\[
e^{A} :=I_{n}+A+\frac{1}{2 !} A^{2}+\frac{1}{3 !} A^{3}+\cdots \in \mathbb{R}^{n \times n}
\]
Matrix power series always converges.
\section{Computing Matrix Exponential Directly}
\begin{itemize}
\item When $A$ is nilpotent.
\item When $A$ is idempotent, i.e., $A^{2}=A$
\item When $A$ is of rank one
\end{itemize}
\section{Computing Matrix Exponential: Method II}
Using the Jordan Canonical Form:
\[
A=T\left[\begin{array}{lll}{J_{1}} & {} \\ {} & {\ddots} & {} \\ {} & {} & {J_{q}}\end{array}\right] T^{-1} \Rightarrow e^{A}=T\left[\begin{array}{ccc}{e^{J_{1}}} & {} & {} \\ {} & {\ddots} & {} \\ {} & {} & {e^{J_{q}}}\end{array}\right] T^{-1}
\]
\section{Computing Matrix Exponential: Other Methods}
\begin{itemize}
\item ``Nineteen dubious ways to compute the exponential of a matrix,'' C.
Moler and C. F. Van Loan, SIAM Review, 20(4): 801-836,1978.
\item ``Nineteen dubious ways to compute the exponential of a matrix,
twenty-five years later,'' C. Moler and C. F. Van Loan, SIAM Review,
45(1): 3-49,2003.\\
Availab at
\[
www.cs.cornell.edu/cv/researchpdf/19ways+.pdf
\]
\item Matlab command $\mathbf{expm}$.
\end{itemize}
\section{Properties of Matrix Exponential}
For any $A\in\mathbb{R}^{n\times n}$
\begin{itemize}
\item $e^{0}=I$
\item $A v=\lambda v \Rightarrow e^{A} v=e^{\lambda} v$
\item $e^{A^{T}}=\left(e^{A}\right)^{T}$
\item $e^{T A T^{-1}}=T e^{A} T^{-1}$ for nonsingular $T \in \mathbb{R}^{n \times n}$
\item $\operatorname{det}\left(e^{A}\right)=e^{\operatorname{tr} A}$
\item If $A, B \in \mathbb{R}^{n \times n}$ commute, i.e., $A B=B A,$ then
\[
e^{A+B}=e^{A} e^{B}=e^{B} e^{A}
\]
\item $\left(e^{A}\right)^{-1}=e^{-A}$
\item If $A$ is skew symmetric $\left(A^{T}=-A\right), e^{A}$ is orthegonal: $\left(e^{A}\right)\left(e^{A}\right)^{T}=1$
\end{itemize}
\section{Baker-Campbell-Hausdorff Formula}
For $X, Y \in \mathbb{R}^{n \times n},$ we have $e^{X+Y} \neq e^{X} \cdot e^{Y}$ unless $X$ and  commute\\
\\
Proposition (Baker-Campbell-Hausdorff Formula):\\
For any $X, Y \in \mathbb{R}^{n \times n},$ we can write
\[
e^{X} e^{Y}=e^{Z}
\]
for some $Z=\log \left(e^{X} e^{Y}\right) \in \mathbb{R}^{3 \times 3}$ given by
\[
Z=X+Y+\frac{1}{2}[X, Y]+\frac{1}{12}[X,[X, Y]]-\frac{1}{12}[Y,[X, Y]]-\frac{1}{24}[Y,[X,[X, Y]]]-\cdots
\]
where $[X, Y] :=X Y-Y X$ is the Lie bracket of $X$ and $Y$.
\section{Matrix Exponential Representation of 3D Rotations}
For $\omega=\left[\omega_{1} \quad \omega_{2} \quad \omega_{3}\right]^{T} \in \mathbb{R}^{3},$ define a skew-symmetric matrix $\Omega$:
\[
\Omega=\left[\begin{array}{ccc}{0} & {-\omega_{3}} & {\omega_{2}} \\ {\omega_{3}} & {0} & {-\omega_{1}} \\ {-\omega_{2}} & {\omega_{1}} & {0}\end{array}\right] \in \mathbb{R}^{3 \times 3}
\]
Then $\Omega_V=\omega \times v$ for $v \in \mathbb{R}^{3},$ where $\times$ denotes crooduct of vectors.\\
\\
Proposition:\\
For any nonzero vector $\omega \in \mathbb{R}^{3}, e^{\Omega} \in \mathbb{R}^{3 \times 3}$ is an orthogonal matrix that represents the rotation around the axis $\omega$ by the angle $||\omega||$.  More precisely,
\[
e^{\Omega}=I_{3}+\frac{\sin (\|\omega\|)}{\|\omega\|} \Omega+\frac{1-\cos (\|\omega\|)}{\|\omega\|^{2}}\left(\omega \omega^{T}-\|\omega\|^{2} I_{3}\right)
\]
$\bullet$ See ``Finite Dimensional Linear Systems'' by Roger Brockett
\section{Example:}
Example: $A=\left[\begin{array}{cc}{\sigma} & {-\omega} \\ {\omega} & {\sigma}\end{array}\right]$
\section{Time Indexed Matrix Exponential}
The following power series converges for all $\lambda \in \mathbb{R}$ and all $t \in \mathbb{R}$:
\[
f(\lambda) :=e^{\lambda t}=1+t \lambda+\frac{1}{2 !} t^{2} \lambda^{2}+\frac{1}{3 !} t^{3} \lambda^{3}+\cdots
\]
For any square matrix $A\in\mathbb{R}^{n\times n}$, define
\[
e^{A t} :=I_{n}+t A+\frac{1}{2 !} t^{2} A^{2}+\frac{1}{3 !} t^{3} A^{3}+\cdots
\]
\begin{itemize}
\item The matrix power series converges for all $A \in \mathbb{R}^{n \times n}$ and all $t \in \mathbb{R}$
\item For fixed $A, e^{A t}$ can be viewed as a matrix-valued function of time $t$
\end{itemize}
\section{Time Derivative of Matrix Exponential}
The scalar function $e^{\lambda t}$ as a function of $t \in \mathbb{R}$ has the derivative:
\[
\frac{d}{d t} e^{\lambda t}=\lambda e^{\lambda t}
\]
Proposition:\\
For fixed $A \in \mathbb{R}^{n \times n}, e^{A t}$ as a matrix-valued function of $t \in \mathbb{R}$ satisfies
\[
\frac{d}{d t} e^{A t}=A \cdot e^{A t}=e^{A t} \cdot A
\]
\section{Other Properties of Matrix Exponential}
For any $A \in \mathbb{R}^{n \times n}$ and any $t \in \mathbb{R}$:
\begin{itemize}
\item $A v=\lambda v \Rightarrow e^{A t} v=e^{\lambda t} v$
\item $e^{A^{T} t}=\left(e^{A t}\right)^{T}$
\item $\operatorname{det}\left(e^{A t}\right)=e^{(\operatorname{tr} A) t}$
\item If $A, B \in \mathbb{R}^{n \times n}$ commute, i.e., $A B=B A,$ then
\[
e^{(A+B) t}=e^{A t} e^{B t}=e^{B t} e^{A t}
\]
\item $e^{A\left(t_{1}+t_{2}\right)}=e^{A t_{1}} e^{A t_{2}}=e^{A t_{2}} e^{A t_{1}}, \forall t_{1}, t_{2} \in \mathbb{R}$
\item $\left(e^{A t}\right)^{-1}=e^{-A t}$
\item If $A$ is skew symmetric, then $e^{A t}$ is orthogonal for all $t$
\end{itemize}
\section{Computing Time-Indexed Matrix Exponential}
Method I: use the definition:
\[
e^{A t} :=I_{n}+t A+\frac{1}{2 !} t^{2} A^{2}+\frac{1}{3 !} t^{3} A^{3}+\cdots
\]
Method II: use the Jordan canonical form:
\[
A=T\left[\begin{array}{lll}{J_{1}} & {} & {} \\ {} & {\ddots} & {} \\ {} & {} & {J_{q}}\end{array}\right] T^{-1} \Rightarrow e^{A t}=T\left[\begin{array}{ccc}{e^{J_{1} t}} & {} & {} \\ {} & {\ddots} & {} \\ {} & {} & {e^{J_{q} t}}\end{array}\right] T^{-1}
\]
Method III: use the following result\\
\\
Proposition:\\
The Laplace transform of $e^{At}$ as a function of time $t$ is
\[
\mathcal{L}\left[e^{A t}\right]=(sI-A)^{-1} \Rightarrow e^{A t} = \mathcal{L}^{-1}\left[(sI-A)^{-1}\right]
\]
\section{Example}
\[
A_{1}=\left[\begin{array}{cc}{0} & {-\omega} \\ {\omega} & {0}\end{array}\right], \quad e^{A_{1} t}=
\]
\[
A_{2}=\left[\begin{array}{ccc}{1} & {1} & {0} \\ {0} & {-2} & {1} \\ {0} & {0} & {-2}\end{array}\right],\left(sI-A_{2}\right)^{-1}=\left[\begin{array}{ccc}{\frac{1}{s-1}} & {\frac{1}{(s-1)(s+2)}}&{\frac{1}{(s-1)(s+2)^{2}}} \\ {0} & {\frac{1}{s+2}}&  {\frac{1}{(s+2)^2}} \\ {0} & {0}& {\frac{1}{s+2}} \end{array}\right]
\]
\end{document}