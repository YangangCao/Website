\documentclass[10pt,a4paper,oneside]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{enumerate}
\usepackage{multirow}
% \usepackage[framed,numbered,autolinebreaks,useliterate]{mcode} 
\date{July 8, 2019}
\author{Baboo J. Cui, Yangang Cao}
\title{Lecture 3: Linear Algebra Review}
\newcommand{\tabincell}[2]{\begin{tabular}{@{}#1@{}}#2\end{tabular}}
\begin{document}
\maketitle
\tableofcontents
\newpage

\section{Vector Spaces}
A (real) vector space $V$ is a set with \textbf{two} operations(also known as close to linear operation):
\begin{itemize}
\item vector sum: $V +V\rightarrow V$
\item scalar multiplication: $\mathbb{R}\times V\rightarrow V$
\end{itemize}
that has the following ($7$) properties:
\begin{enumerate}
\item commutativity: $x+y=y+x, \forall x, y \in V$
\item associativity: $(x+y)+z=x+(y+z), \forall x, y, z \in V$
\item zero element: $\exists !0 \in V$ such that $0+x=x, \forall x \in V$, $\exists!$ means \textbf{only exist}
\item inverse: $\forall x \in V, \exists(-x) \in V$ such that $x+(-x)=0$
\item associativity in scalar product: $(\alpha \beta) x=\alpha(\beta x), \forall \alpha, \beta \in \mathbb{R}, x \in V$
\item distributivity: $\alpha(x+y)=\alpha x+\alpha y, \forall a \in \mathbb{R}, x, y \in V$
\item distributivity: $(\alpha+\beta) x=\alpha x+\beta x, \forall \alpha, \beta \in \mathbb{R}, x \in V$
\end{enumerate}
The properties implies that:
\[
1 \cdot x=x \text{ and } 0 \cdot \vec{x}=\vec{0}, \forall x \in V
\]

\subsection{Example of Vector Spaces}
\begin{itemize}
\item $\mathbb{R}^n$
\item $\mathbb{R}^{m \times n}$
\item $P_{n}$ : the set of all polynomials in $\lambda$ with degree up to $n$, note that DOF is $n+1$
\item $\mathcal{F}\left(\mathcal{I} ; \mathbb{R}^{n}\right)$: Set of all mappings from an index set $\mathcal{I}$ to $\mathbb{R}^{n}$
\item set of all differentiable function $f : \mathbb{R}_{+} \rightarrow \mathbb{R}$
\item set of all square integrable function $f : \mathbb{R}_{+} \rightarrow \mathbb{R}$
\item set of all solutions to an autonomous LTI system
\end{itemize}

\subsection{Subspaces and Product Spaces}
\begin{itemize}
	\item \textbf{subspace}: $W$ is a subspace of vector space $V$ if $W \subset V$ and $W$ itself is a vector space under the same vector sum and scalar multiplication operations
	\item \textbf{product space}: given two vector spaces $V_{1}$ and $V_{2},$ their \textbf{direct product} is the vector
	space $V_{1} \times V_{2} :=\left\{\left(v_{1}, v_{2}\right) | v_{1} \in V_{1}, v_{2} \in V_{2}\right\}$, essentially, link two vectors together
\end{itemize}



\subsection{Bases and Dimension of Vector Spaces}
$\{v_{1}, \ldots, v_{k}\}$ in vector space $V$ are linearly independent if for $\alpha_{1}, \ldots, \alpha_{k} \in \mathbb{R}$,
\[
\alpha_{1} v_{1}+\cdots+\alpha_{k} v_{k}=0 \quad \Rightarrow \quad \alpha_{1}=\cdots=\alpha_{k}=0
\]
A set of vectors $\left\{v_{1}, \ldots, v_{k}\right\}$ is a \textbf{basis} of the vector space $V$ if
\begin{itemize}
\item $v_{1}, \ldots, v_{k}$ are linearly independent in $V$
\item $V=\operatorname{span}\left\{v_{1}, \ldots, v_{k}\right\}$
\end{itemize}
Or equivalently,
\begin{itemize}
\item each $v \in V$ has a \textbf{unique} expression $v=\alpha_{1} v_{1}+\cdots+\alpha_{k} v_{k}$
\item $\left(\alpha_{1}, \ldots, \alpha_{k}\right)$ is the coordinate of $v$ in this basis
\end{itemize}
The \textbf{dimension} of a vector space $V$ is the number of vectors in any of its basis, and is denoted $\dim V$.

\section{Linear Maps}
$A$ map $f : V \rightarrow W$ between two vector spaces $V$ and $W$ is linear if
\[
f\left(\alpha_{1} v_{1}+\alpha_{2} v_{2}\right)=\alpha_{1} f\left(v_{1}\right)+\alpha_{2} f\left(v_{2}\right)
\]
\begin{itemize}
\item A linear map $f : V \rightarrow W$ must map $0 \in V$ to $0\in W$, also known as ZIZO
\item The composition of two linear maps $f:V\rightarrow W$ and $g:W\rightarrow U$ is also linear: $g \circ f : v \in V \mapsto g(f(v)) \in U$
\end{itemize}

\subsection{Null Spaces and Images of Linear Maps}
\begin{itemize}
	\item \textbf{null space}: the null space of a linear map $f : V \rightarrow W$ is $\mathcal{N}(f) :=\{v \in V | f(v)=0\}$, note that $\mathcal{N}(f)$ is a subspace of $V$
	\item \textbf{image(range)}: the image (or range) of a linear map $f : V \rightarrow W$ is
	\[
	\mathcal{R}(f) :=\{w \in W | w=f(v) \text { for some(some is enough) } v \in V\}
	\]
	$\mathcal{R}(f)$ is a subspace of $W$.
\end{itemize}

\subsection{Injective (One-To-One) Linear Maps}
A linear map $f : V \rightarrow W$ is \textbf{injective} (one-to-one) if for all $v_{1}, v_{2} \in V$,
\[
f\left(v_{1}\right)=f\left(v_{2}\right) \Rightarrow v_{1}=v_{2}
\]
Equivalent definitions:
\begin{itemize}
\item $f$ maps different vectors to different vectors
\item $f$ maps linearly independent vectors to linearly independent vectors
\item $\mathcal{N}(f)=\{0\}$
\end{itemize}
Matrix $A \in \mathbb{R}^{m \times n}$ considered as a linear map $\mathbb{R}^{n}$ to $\mathbb{R}^{m}$ has null space
\[
\mathcal{N}(A)=\left\{x \in \mathbb{R}^{n} | A x=0\right\}
\]
$A \in \mathbb{R}^{m \times n}$ is \textbf{one-to-one} if and only if
\begin{itemize}
\item Columns of $A$ are linearly independent
\item Rows of $A$ span $\mathbb{R}^n$
\item $A$ has rank $n$ (full column rank)
\item $A$ has a left inverse: $\exists B \in \mathbb{R}^{n \times m}$ such that $B A=I_{n}$
\item $\det(A^TA)\neq0$
\end{itemize}

\subsection{Surjective (Onto) Linear Maps}
A linear map $f:V\rightarrow W$ is surjective (onto) if $\mathcal{R}(f)=W$, or equivalently, if for any $w \in W, w=f(v)$ for some $v \in V$. Matrix $A \in \mathbb{R}^{m \times n}$ considered as a linear map $\mathbb{R}^{n}$ to $\mathbb{R}^{m}$ has range space
$\mathcal{R}(A)=\left\{A x \in \mathbb{R}^{m} | x \in \mathbb{R}^{n}\right\}$, $A\in\mathbb{R}^{m\times n}$ is \textbf{onto} if and only if
\begin{itemize}
\item rows of $A$ are linearly independent
\item columns of $A$ span $\mathbb{R}^{m}$
\item rank of $A$ is $m$ (full row rank)
\item $A$ has a right inverse $\exists B \in \mathbb{R}^{n \times m}$ such that $A B=I_{m}$
\end{itemize}

\subsection{Bijective (Invertible) Linear Maps}
A linear map $f : V \rightarrow W$ is bijective (invertible) if it is both one-to-one and onto. Its inverse is the unique map $f^{-1} : W \rightarrow V$ such that $f \circ f^{-1}=i d_W$ and $f^{-1} \circ f=i d_V$, $V$ and $W$ must have the same dimension, a matrix $A\in \mathbb{R}^{n\times n}$ is invertible if $A : \mathbb{R}^{n} \rightarrow \mathbb{R}^{n}$ is bijective, it is equivalent to: 
\begin{itemize}
\item columns (or rows) of $A$ form a basis of $\mathbb{R}^{n}$
\item $A$ has inverse $A^{-1}$ with $A A^{-1}=A^{-1} A=I_{n}$
\item $\mathcal{N}(A)=\{0\}$
\item $\mathcal{R}(A)=\mathbb{R}^{n}$
\item $\det A \neq 0$
\end{itemize}

\section{Matrix}
\subsection{Matrix Rank}
The rank of a matrix $A \in \mathbb{R}^{m \times n}$ is its maximum number of linearly independent  columns (or rows), or equivalently, $\operatorname{dim} \mathcal{R}(A)$, it has the following properties:
\begin{itemize}
\item $\operatorname{Rank}(A) \leq \min (m, n)$
\item $\operatorname{Rank}(A)=\operatorname{Rank}\left(A^{T}\right)$
\item $\operatorname{Rank}(A)+\operatorname{dim} \mathcal{N}(A)=n$ (conservation of dimension)
\end{itemize}
{\bfseries Full rank} matrix $A \in \mathbb{R}^{m \times n}: \operatorname{Rank}(A)=\min (m, n)$
\begin{itemize}
\item (for skinny matrices) independent column or injective maps
\item (for fat matrices) independent rows or surjective maps
\item (for square matrices) nonsingular or bijective maps
\end{itemize}

\subsection{Matrix Transpose}
When $A \in \mathbb{R}^{m \times n}$ is considered as a linearmap from $\mathbb{R}^{n}$ to $\mathbb{R}^{m}$ , its
transpose $A^{T} \in \mathbb{R}^{n \times m}$ is a linear map from $\mathbb{R}^{m}$ back to $\mathbb{R}^{n}$\\
\\The following are equivalent:
\begin{itemize}
\item $A$ is one-to-one
\item $A^{T}$ is onto
\item $\operatorname{det} A^{T} A \neq 0$
\item $A^{T} A \in \mathbb{R}^{n \times n}$ is bijective
\end{itemize}
The following are equivalent:
\begin{itemize}
\item $A$ is onto
\item $A^{T}$ is one-to-one
\item $\operatorname{det} A A^{T} \neq 0$
\item $A A^{T} \in \mathbb{R}^{m \times m}$ is bijective
\end{itemize}
More generally, for any $A\in \mathbb{R}^{m\times n}$
\begin{itemize}
\item $\mathcal{R}\left(A^{T}\right)=\mathcal{N}(A)^{\perp}$
\item $\mathcal{N}\left(A^{T}\right)=\mathcal{R}(A)^{\perp}$
\end{itemize}
\section{Inner Product on Euclidean Space}
For $x, y \in \mathbb{R}^{n},$ their inner product is
\[
\langle x, y\rangle := x^{T} y=x_{1} y_{1}+\cdots+x_{n} y_{n}
\]
For $x, y, z \in \mathbb{R}^{n}$:
\begin{itemize}
\item $\langle x, y\rangle=\langle y, x\rangle$
\item $\langle\alpha x, y\rangle=\alpha\langle x, y\rangle$
\item $\langle x+y, z\rangle=\langle x, z\rangle+\langle x, y\rangle$
\item $\langle x, x\rangle=\|x\|^{2} \geq 0$, where $||x||$ is the Euclidean norm of $x$:
\[
\|x\| :=\sqrt{x^{T} x}=\sqrt{x_{1}^{2}+\cdots+x_{n}^{2}}
\]
\end{itemize}
Theorem (Cauchy-Schwartz Inequality):
\[
|\langle x, y\rangle| \leq\|x\| \cdot\|y\|, \quad \forall x, y \in \mathbb{R}^{n}
\]
\section{Finite Dimensional Vector Space vs. $\mathbb{R}^n$}
Theorem:\\
There is a bijection between any finite dimensional vector space $V$ and $\mathbb{R}^n$ with $n=\dim V$.\\
\\
Proof:\\
$\rho_{V} : v \in V \mapsto$ its coordinate in a basis $\left\{v_{1}, \ldots, v_{n}\right\}$ of $V$.
\section{Matrix Representation of Linear Maps}
Theorem:\\
Any linear $\operatorname{map} f : V \rightarrow W$ between two finite dimensional vector spaces can be represented as a matrix $A\in\mathbb{R}^{m\times n}$ with $n=\dim V, m=\dim W$.
\section{Examples}
\begin{itemize}
\item For $A\in\mathbb{R}^{m\times n}$, the map $x \in \mathbb{R}^{n} \mapsto A x \in \mathbb{R}^{m}$ viewed in standard basis
\[
\mathbb{R}^{n} \stackrel{A}{\longrightarrow} \mathbb{R}^{m}
\]
\item $\frac{d}{d \lambda} : p(\lambda) \in \mathcal{P}_{n} \mapsto \frac{d p(\lambda)}{d \lambda} \in \mathcal{P}_{n-1}$
\end{itemize}
\section{Determinant and Inverse of Square Matrices}
For any square matrix $A\in\mathbb{R}^{n\times n}$, its {\bfseries determinant} is defined recursively as
\[
\operatorname{det} A :=\sum_{i=1}^{n} a_{i j} c_{i j}
\]
\begin{itemize}
\item $a_{i j} :$ entry of $A$ on row $i$ and column $j$
\item $c_{i j}=(-1)^{i+j}$ det $M_{i j} :$ cofactor corresponding to $a_{i j}$
\item measures the volume amplification of linear map $A$
\end{itemize}
For nonsingular matrices ($\det A\neq0$), the {\bfseries inverse matrix} of $A\in\mathbb{R}^{n\times n}$ is the unique matrix $A^{-1}\in\mathbb{R}^{n\times n}$ satisfying $AA^{-1}=A^{-1}A=I_n$:
\[
A^{-1}=\frac{\operatorname{Adj} A}{\operatorname{det} A}=\frac{\left[c_{i j}\right]^{T}}{\operatorname{det} A}
\]
\section{Spectrum of Square Matrices}
The characteristic polynomial of a square matrix $A \in \mathbb{R}^{n \times n}$ is
\[
\chi_{A}(\lambda) :=\operatorname{det}\left(\lambda I_{n}-A\right) \in \mathcal{P}_{n}
\]
Definition (Spectrum of $A$):\\
The $n$ roots (counting multiplicity, possibly complex) of $\chi_{A}(\lambda)$ are the eigenvalues of $A$. The {\bfseries spectrum} of $A$ is the set $\sigma(A)$ of all its eigenvalues.\\
\\
For each eigenvalue $\lambda_i\in\mathbb{C}$ of $A$,
\begin{itemize}
\item $v_{i} \in \mathbb{C}^{n}$ is called a (right) eigenvector if $A v_{i}=\lambda_{i} v_{i}$
\item $w_{i} \in \mathbb{C}^{n}$ is called a left eigenvector if $w_{i}^{T} A=\lambda_{i} w_{i}^{T}$
\end{itemize}
Example:
\[
\left[\begin{array}{cc}{0.4} & {0.6} \\ {0.7} & {0.3}\end{array}\right] \qquad\left[\begin{array}{cc}{a} & {b} \\ {-b} & {a}\end{array}\right]
\]
\section{Change of Basis in $\mathbb{R}^n$}
A vector $x=[x_1\cdots x_n]^T\in R^n$ in standard basis has the following coordinate in new basis \{$t_1,\cdots,t_n$\}:
\[
\tilde{x}=T^{-1} x=\left[\begin{array}{lll}{t_{1}} & {\cdots} & {t_{n}}\end{array}\right]^{-1} x
\]
$A \in \mathbb{R}^{n \times n}$ as a linear map in standard basis when viewed in a different
basis $\left\{t_{1}, \ldots, t_{n}\right\}$ has matrix representation:
\[
\tilde{A}=T^{-1} A T
\]
\[
\begin{array}{l}{\mathbb{R}^{n} \stackrel{A}{\longrightarrow} \mathbb{R}^{n}} \\ {\downarrow T^{-1} \quad\downarrow T^{-1}} \\ {\mathbb{R}^{n} \stackrel{\tilde{A}}{\longrightarrow} \mathbb{R}^{n}}\end{array}
\]
\section{Similarity Transformations}
Two matrices $A, \tilde{A} \in \mathbb{R}^{n \times n}$ are {\bfseries similar} if there exists a nonsingular matrix $T \in \mathbb{R}^{n \times n}$ such that
\[
\tilde{A}=T^{-1} A T
\]
\begin{itemize}
\item Representing the same linear map viewed in different bases
\item Determinant is invariant: $\det A=\det \tilde{A}$
\item Spectrum is invariant: $\sigma(A)=\sigma(\tilde{A})$
\end{itemize}
\section{Diagonalizable Matrices}
Definition (Diagonalizable Matrix):\\
Matrix $A\in\mathbb{R}^{n\times n}$ is called diagonalizable if there exists a nonsingular matrix $T\in\mathbb{C}^{n\times n}$ such that $T^{-1}AT=\Lambda\in\mathbb{C}^{n\times n}$ is diagonal.
\begin{itemize}
\item Diagonal entries of $\Lambda=\operatorname{diag}\left(\lambda_{1}, \ldots, \lambda_{n}\right)$ are the eigenvalues of $A$
\item Column of $T=\left[\begin{array}{ccc}{v_{1}} & {\cdots} & {v_{n}}\end{array}\right]$ are the right eigenvectors of $A$
\item Rows of $T^{-1}=\left[\begin{array}{ccc}{w_{1}} & {\dots} & {w_{n}}\end{array}\right]^{T}$ are (the transpose of) the left
eigenvectors of $A$
\end{itemize}
\section{Dyadic Expansion of Diagonalizable Matrices}
Diagonalizable matrix $A \in \mathbb{R}^{n \times n}$ with $T^{-1} A T=\Lambda$ can be decompesed as
\[
A=\lambda_{1} v_{1} w_{1}^{T}+\lambda_{2} v_{2} w_{2}^{T}+\cdots \lambda_{n} v_{n} w_{n}^{T}
\]
$\bullet$ Sum of $n$ rank-one matrices\\
Example:
\[
\underbrace{\left[\begin{array}{cc}{\frac{7}{13}} & {\frac{6}{13}} \\ {1} & {-1}\end{array}\right]}_{T^{-1}} \underbrace{\left[\begin{array}{cc}{0.4} & {0.6} \\ {0.7} & {0.3}\end{array}\right]}_{A} \underbrace{\left[\begin{array}{cc}{1} & {\frac{6}{13}} \\ {1} & {-\frac{7}{13}}\end{array}\right]}_{T}=\underbrace{\left[\begin{array}{cc}{1} & {0} \\ {0} & {0.3}\end{array}\right]}_{\Lambda}
\]
\section{Jordan Canonical Form}
Theorem (Jordan Canonical Form):\\
For any $A\in\mathbb{R}^{n\times n}$, there exists a nonsingular $T\in\mathbb{C}^{n\times n}$ such that
\[
T^{-1} A T=J=\left[\begin{array}{lll}{J_{1}} & {} & {} \\ {} & {\ddots} & {} \\ {} & {} & {J_{q}}\end{array}\right], \quad J_{i}=\left[\begin{array}{cccc}{\lambda_{i}} & {1} & {} & {} \\ {} & {\ddots} & {\ddots} & {} \\ {} & {} & {\ddots} & {1} \\ {} & {} & {} & {\lambda_{i}}\end{array}\right] \in \mathbb{C}^{n_{i} \times n_{i}}
\]
\begin{itemize}
\item Unique up to permutation of Jordan blocks
\item Diagonalizable matrices are special cases with all $n_i=1$
\end{itemize}
Definition (Algebraic and Geometric Multiplicity):
The {\bfseries algebraic multiplicity} of an eigenvalue $\lambda_i$ is the sum of the sizes of all Jordan blocks correpsonding to it; its {\bfseries geometric multiplicity} is the number of all such Jordan blocks.
\section{Geometric Characterization of Jordan Block Sizes}
Given $A\in R^n$ with an eigenvalue $\lambda$, construct a cascade of subspaces:
\[
\mathcal{N}\left(A-\lambda I_{n}\right) \subset \mathcal{N}\left(A-\lambda I_{n}\right)^{2} \subset \mathcal{N}(A-\lambda I_n)^{3} \subset \cdots
\]
{\bfseries Fact}: the geometric multiplicity ( $\#$ of Jordan blocks) of eigenvalue $\lambda$ is
\begin{itemize}
\item $\operatorname{dim} \mathcal{N}\left(A-\lambda I_{n}\right)$
\item The number of linearly independent eigenvectors corresponding to $\lambda$
\end{itemize}
In general, $\#$ of Jordan blocks of $\lambda$ with size at least $k$ is
\[
\operatorname{dim} \mathcal{N}\left(A-\lambda I_{n}\right)^{k}-\operatorname{dim} \mathcal{N}\left(A-\lambda I_{n}\right)^{k-1}
\]
\section{Example}
Suppose that $\lambda_{1}$ is an eigenvalue of the matrix $A \in \mathbb{R}^{10 \times 10}$ , and that the dimensions of the null space of $\left(A-\lambda_{1} I\right)^{k}$ for $k=1,2,3,4$ and 5 are:
$3,6,8,9$ and $9,$ respectively.
\begin{itemize}
\item How many Jordan blocks are associated with $\lambda_{1} ?$
\item What are the sizes of these Jordan blocks?
\item Does $A$ have other eigenvalues? If yes, what are their multiplicities?
\end{itemize}
\section{Generalized Eigenvectors}
Definition (Generalized Eigenvector):\\
Vector $v$ is a generalized eigenvector of $A$ of grade $d$ if
\[
v \in \mathcal{N}(A-\lambda I_n)^{d} \quad \text { and } \quad v \notin \mathcal{N}(A-\lambda I_n)^{d-1}
\]
$\bullet$ When $d=1$, this reduces to the definition of eigenvectors\\
\\
Example $(d=3) :$ A chain of generalized eigenvectors of length 3:
\[
\{v_{3} :=v, v_{2} :=\left(A-\lambda_{n}\right) v, v_{1} :=\left(A-\lambda l_{n}\right)^{2} v\}
\]
which satisfies
\[
A v_{1}=\lambda v_{1}, A v_{2}=v_{1}+\lambda v_{2}, A v_{3}=v_{2}+\lambda v_{3}
\]
$v_{1}, v_{2}, v_{3}$ are the columns of $T$ corresponding to a Jordan block of size 3
\section{Real Jordan Canonical Form}
Theorem (Real Jordan Canonical Form):
For any $A \in \mathbb{R}^{n \times n}$ , there exists a nonsingular $T \in \mathbb{R}^{n \times n}$ such that:
\[
T^{-1} A T=J=\left[\begin{array}{ccc}{J_{1}} \\ {} & {\ddots} \\ {} & {} &{} {J_{q}}\end{array}\right],
\]
where $J_{i}=\left[\begin{array}{cccc}{\lambda_{i}} & {1} & {} & {} \\ {} & {\ddots} & {\ddots} & {} \\ {} & {} & {\ddots} & {1} \\ {} & {} & {} & {\lambda_{i}}\end{array}\right]$ for real $\lambda_i$ and $J_{i}=\left[\begin{array}{cccc}{C_{i}} & {I_{2}} & {} & {} \\ {} & {\ddots} & {\ddots} & {} \\ {} & {} & {\ddots} & {I_{2}} \\ {} & {} & {} & {C_{i}}\end{array}\right]$ with $C_{i}=\left[\begin{array}{ll}{a_{i}} & {b_{i}} \\ {-b_{i}} & {a_{i}}\end{array}\right]$ for complex $\lambda_{i}=a_{i}+b_{i} \sqrt{-1}$.
\end{document}